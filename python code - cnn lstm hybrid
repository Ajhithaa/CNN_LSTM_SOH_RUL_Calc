import numpy as np
import pandas as pd
import os

# Ensure TensorFlow is available
try:
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import (
        Input, Conv1D, LSTM, Dense, Dropout, BatchNormalization,
        GlobalAveragePooling1D, Reshape
    )
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.losses import MeanSquaredError
    from tensorflow.keras.callbacks import EarlyStopping
except ModuleNotFoundError as e:
    raise ModuleNotFoundError("TensorFlow is not installed. Please install it using pip install tensorflow") from e

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error

def load_data(folder_path):
    """
    Load all CSV files from the specified folder and concatenate them into a single DataFrame.
    """
    all_files = os.listdir(folder_path)
    df_list = []
    for file in all_files:
        if file.endswith(".csv"):
            file_path = os.path.join(folder_path, file)
            df = pd.read_csv(file_path)
            df_list.append(df)

    if not df_list:
        raise ValueError("No CSV files found in the provided folder.")

    return pd.concat(df_list, ignore_index=True)

def preprocess_data(df):
    """
    Perform data cleaning and preprocessing by:
      - Replacing infinite values with NaN and filling missing values with column mean.
      - Extracting features and target.
      - Scaling features and target using MinMaxScaler.
      - Reshaping the feature array to include the channel dimension.
    """
    # Clean the dataframe first
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.fillna(df.mean(), inplace=True)

    # Check that required columns exist
    required_cols = ['terminal_voltage', 'terminal_current', 'temperature',
                     'charge_current', 'charge_voltage', 'capacity']
    for col in required_cols:
        if col not in df.columns:
            raise ValueError(f"Column '{col}' not found in the dataset.")

    # Extract features and target
    features = df[['terminal_voltage', 'terminal_current', 'temperature', 'charge_current', 'charge_voltage']].values
    target = df['capacity'].values  # Using 'capacity' as target

    # Scale features and target to [0, 1]
    scaler_x = MinMaxScaler()
    features_scaled = scaler_x.fit_transform(features)

    scaler_y = MinMaxScaler()
    target_scaled = scaler_y.fit_transform(target.reshape(-1, 1)).flatten()

    # Reshape the features to add a channel dimension (samples, timesteps, channels)
    X = features_scaled.reshape(features_scaled.shape[0], features_scaled.shape[1], 1)

    return X, target_scaled, scaler_y

def build_model(input_shape):
    """
    Create and compile the CNN-LSTM model.
    """
    inputs = Input(shape=input_shape)

    # Convolutional block
    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same',
               kernel_regularizer=tf.keras.regularizers.l2(0.001))(inputs)
    x = BatchNormalization()(x)
    x = Dropout(0.2)(x)

    # Global pooling to reduce the temporal dimension
    x = GlobalAveragePooling1D()(x)

    # Reshape data for LSTM layer
    x = Reshape((1, -1))(x)

    # LSTM layer for learning temporal dependencies
    x = LSTM(50, return_sequences=False)(x)

    # Dense layers for final prediction
    x = Dense(50, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(1, activation='linear')(x)

    model = Model(inputs, outputs)

    # Compile with gradient clipping enabled
    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)
    model.compile(optimizer=optimizer, loss=MeanSquaredError(), metrics=['mae'])

    return model

def calculate_soh(y_actual):
    """
    Calculate the State of Health (SOH), normalizing current capacity by the first measured capacity.
    """
    initial_capacity = y_actual[0]
    soh = (y_actual / initial_capacity) * 100
    return soh

def calculate_rul(y_actual):
    """
    Calculate the Remaining Useful Life (RUL) in cycles by subtracting the current capacity from the initial value.
    """
    initial_capacity = y_actual[0]
    rul = np.maximum(initial_capacity - y_actual, 0)
    return rul

def train_and_evaluate(folder_path):
    """
    Load the data, preprocess it, build the model, then train and evaluate the model.
    """
    # Load data from CSVs
    df = load_data(folder_path)
    print("Data loaded successfully. Shape:", df.shape)

    # Preprocess the data
    X, y, scaler_y = preprocess_data(df)
    print("Data preprocessed. X shape:", X.shape, "y shape:", y.shape)

    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print("Train-Test split completed. X_train shape:", X_train.shape, "X_test shape:", X_test.shape)

    # Build the CNN-LSTM model
    model = build_model(input_shape=(X.shape[1], X.shape[2]))
    print("Model built successfully.")

    # Early stopping callback to prevent overfitting
    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

    # Train the model
    print("Training the model...")
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_split=0.2,
        callbacks=[early_stopping],
        verbose=1
    )

    # Perform predictions on the test set
    y_pred_scaled = model.predict(X_test)
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_actual = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()

    # Calculate SOH and RUL using the actual capacity values
    soh = calculate_soh(y_actual)
    rul = calculate_rul(y_actual)

    # Compute evaluation metrics
    mape = mean_absolute_percentage_error(y_actual, y_pred)
    rmse = np.sqrt(mean_squared_error(y_actual, y_pred))
    r2 = r2_score(y_actual, y_pred)
    mae = mean_absolute_error(y_actual, y_pred)

    print("\nEvaluation Metrics:")
    print(f"MAPE: {mape * 100:.2f}%")
    print(f"RMSE: {rmse:.4f}")
    print(f"RÂ² Score: {r2:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"Average SOH: {soh.mean():.2f}%")
    print(f"Average RUL: {rul.mean():.2f} cycles")

if __name__ == '__main__':

    # Update this folder path to point to the directory containing your CSV files.
    folder_path = "/content/drive/MyDrive/soh_test"
    train_and_evaluate(folder_path)
